{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b8afc9-02cd-49a2-a0e4-469d140b6c65",
   "metadata": {},
   "source": [
    "# Example Usage of EQTransformer from seisbench\n",
    "This notebook is mostly an uncommented version of the “03a_training_phasenet” notebook from the seisbench repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f37cd84-df3b-4756-b6c7-1528f5f6babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seisbench.generate as sbg\n",
    "import seisbench.models as sbm\n",
    "\n",
    "from seisbench.data import WaveformDataset\n",
    "from seisbench.models import EQTransformer\n",
    "from seisbench.util import worker_seeding\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da776276-4e09-4c49-ac92-51641fb67eea",
   "metadata": {},
   "source": [
    "Load the 100Samples dataset from the EQTransformer repository but transformed into seisbench-compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4991ae28-c6a5-438a-af9e-910afed559ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = WaveformDataset(\"data/STEAD/example/seisbench\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783e474-5c0b-4838-b573-4ec6de0761a3",
   "metadata": {},
   "source": [
    "Instantiate a version of seisbench’s EQTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a50c3a5-7107-4e33-ae44-74c67df32ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eqt = EQTransformer()\n",
    "eqt = sbm.EQTransformer().from_pretrained(\"original\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0a06c-f802-448c-8c1c-f65edbacd563",
   "metadata": {},
   "source": [
    "This part is slightly weird, but the *data* contains information about what is training, validation, and test, already.\n",
    "In this case, I just randomly designated 81 samples as training data, 9 as dev/validation data, and the remaining 10 as test data; see also the “Convert Example Data to Seisbench Format” notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7242a5-a7ef-430d-9408-6f1897c7d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = example_data.train_dev_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d2a30-3ff4-420f-96fd-6199725b4b2f",
   "metadata": {},
   "source": [
    "Rename the label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40381d3d-082c-4d1b-8d62-423672033871",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_dict = {\n",
    "    \"trace_p_arrival_sample\": \"P\",\n",
    "    \"trace_s_arrival_sample\": \"S\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416e51f-ffda-4f60-a19e-28a61f7649d3",
   "metadata": {},
   "source": [
    "Perform (minimal) data augmentation.  I assume (but did not test) that `ChangeDtype` is necessary for the code to run; `ProbabilisticLabeller` provides the probability curves below, but I do not know how they are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12fa2458-da1f-4c0d-a2dd-02a264744848",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = sbg.GenericGenerator(train)\n",
    "dev_generator = sbg.GenericGenerator(train)\n",
    "\n",
    "augmentations = [\n",
    "    sbg.ChangeDtype(np.float32),\n",
    "    sbg.ProbabilisticLabeller(label_columns=phase_dict, sigma=30, dim=0)\n",
    "]\n",
    "\n",
    "train_generator.add_augmentations(augmentations)\n",
    "dev_generator.add_augmentations(augmentations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6840aaa6-ae95-43be-ae94-1908b4bb46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 #\n",
    "num_workers = 1  # The number of threads used for loading data\n",
    "\n",
    "train_loader = DataLoader(train_generator, batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=worker_seeding)\n",
    "dev_loader = DataLoader(dev_generator, batch_size=batch_size, shuffle=False, num_workers=num_workers, worker_init_fn=worker_seeding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace import Laplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Laplace(eqt, 'regression', subset_of_weights='all', hessian_structure='diag')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robot/anaconda3/envs/dslab/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([81, 3, 6000])) that is different to the input size (torch.Size([3, 6000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "BackPACK extension expects a backpropagation quantity but it is None. Module: Component order:\tZNE\nSeisBench model\t\tEQTransformer\n\nEQTransformer(\n  (encoder): Encoder(\n    (convs): ModuleList(\n      (0): Conv1d(3, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      (1): Conv1d(8, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (3): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (5): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n    )\n    (pools): ModuleList(\n      (0-6): 7 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (res_cnn_stack): ResCNNStack(\n    (members): ModuleList(\n      (0-3): 4 x ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (4): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n      (5): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (6): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n    )\n  )\n  (bi_lstm_stack): BiLSTMStack(\n    (members): ModuleList(\n      (0-2): 3 x BiLSTMBlock(\n        (lstm): CustomLSTM(\n          (cell_f): ActivationLSTMCell()\n          (cell_b): ActivationLSTMCell()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (transformer_d0): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (transformer_d): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (decoder_d): Decoder(\n    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n    (convs): ModuleList(\n      (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n    )\n  )\n  (conv_d): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  (dropout): Dropout(p=0.1, inplace=False)\n  (pick_lstms): ModuleList(\n    (0-1): 2 x CustomLSTM(\n      (cell_f): ActivationLSTMCell()\n    )\n  )\n  (pick_attentions): ModuleList(\n    (0-1): 2 x SeqSelfAttention()\n  )\n  (pick_decoders): ModuleList(\n    (0-1): 2 x Decoder(\n      (upsample): Upsample(scale_factor=2.0, mode='nearest')\n      (convs): ModuleList(\n        (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n        (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n        (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n        (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n        (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n        (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      )\n    )\n  )\n  (pick_convs): ModuleList(\n    (0-1): 2 x Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  )\n), Extension: <backpack.extensions.secondorder.diag_ggn.DiagGGNExact object at 0x7eff04607050>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/robot/git/Earthquake_Monitoring/EQTransformer Example_laplace.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/robot/git/Earthquake_Monitoring/EQTransformer%20Example_laplace.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m la\u001b[39m.\u001b[39;49mfit(train_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/laplace/baselaplace.py:378\u001b[0m, in \u001b[0;36mParametricLaplace.fit\u001b[0;34m(self, train_loader, override)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    377\u001b[0m X, y \u001b[39m=\u001b[39m tmp[\u001b[39m'\u001b[39m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device), tmp[\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n\u001b[0;32m--> 378\u001b[0m loss_batch, H_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_curv_closure(X\u001b[39m.\u001b[39;49mfloat(), y\u001b[39m.\u001b[39;49mfloat(), N)\n\u001b[1;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_batch\n\u001b[1;32m    380\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mH \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m H_batch\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/laplace/baselaplace.py:961\u001b[0m, in \u001b[0;36mDiagLaplace._curv_closure\u001b[0;34m(self, X, y, N)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_curv_closure\u001b[39m(\u001b[39mself\u001b[39m, X, y, N):\n\u001b[0;32m--> 961\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend\u001b[39m.\u001b[39;49mdiag(X, y, N\u001b[39m=\u001b[39;49mN)\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/laplace/curvature/backpack.py:164\u001b[0m, in \u001b[0;36mBackPackGGN.diag\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m context\u001b[39m.\u001b[39mset_module_extension(EQTransformer, ScaleModuleBatchGrad())\n\u001b[1;32m    163\u001b[0m \u001b[39mwith\u001b[39;00m backpack(context):\n\u001b[0;32m--> 164\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    165\u001b[0m dggn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_diag_ggn()\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactor \u001b[39m*\u001b[39m loss\u001b[39m.\u001b[39mdetach(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactor \u001b[39m*\u001b[39m dggn\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/torch/utils/hooks.py:220\u001b[0m, in \u001b[0;36mBackwardHook.setup_output_hook.<locals>.fn.<locals>.hook\u001b[0;34m(_, grad_output)\u001b[0m\n\u001b[1;32m    218\u001b[0m grad_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pack_with_none([], [], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_inputs)\n\u001b[1;32m    219\u001b[0m \u001b[39mfor\u001b[39;00m user_hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_hooks:\n\u001b[0;32m--> 220\u001b[0m     res \u001b[39m=\u001b[39m user_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule, grad_inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad_outputs)\n\u001b[1;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(el \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m res)):\n\u001b[1;32m    222\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBackward hook for Modules where no input requires \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    223\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mgradient should always return None or None for all gradients.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/backpack/__init__.py:209\u001b[0m, in \u001b[0;36mhook_run_extensions\u001b[0;34m(module, g_inp, g_out)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m debug:\n\u001b[1;32m    208\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[DEBUG] Running extension\u001b[39m\u001b[39m\"\u001b[39m, backpack_extension, \u001b[39m\"\u001b[39m\u001b[39mon\u001b[39m\u001b[39m\"\u001b[39m, module)\n\u001b[0;32m--> 209\u001b[0m     backpack_extension(module, g_inp, g_out)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m debug:\n\u001b[1;32m    212\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[DEBUG] Running extension hook on\u001b[39m\u001b[39m\"\u001b[39m, module)\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/backpack/extensions/backprop_extension.py:127\u001b[0m, in \u001b[0;36mBackpropExtension.__call__\u001b[0;34m(self, module, g_inp, g_out)\u001b[0m\n\u001b[1;32m    125\u001b[0m module_extension \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_module_extension(module)\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m module_extension \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     module_extension(\u001b[39mself\u001b[39;49m, module, g_inp, g_out)\n",
      "File \u001b[0;32m~/anaconda3/envs/dslab/lib/python3.11/site-packages/backpack/extensions/module_extension.py:106\u001b[0m, in \u001b[0;36mModuleExtension.__call__\u001b[0;34m(self, extension, module, g_inp, g_out)\u001b[0m\n\u001b[1;32m     98\u001b[0m bp_quantity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_backproped_quantity(\n\u001b[1;32m     99\u001b[0m     extension, module\u001b[39m.\u001b[39moutput, delete_old_quantities\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    102\u001b[0m     extension\u001b[39m.\u001b[39mexpects_backpropagation_quantities()\n\u001b[1;32m    103\u001b[0m     \u001b[39mand\u001b[39;00m bp_quantity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_loss(module)\n\u001b[1;32m    105\u001b[0m ):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBackPACK extension expects a backpropagation quantity but it is None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule: \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m, Extension: \u001b[39m\u001b[39m{\u001b[39;00mextension\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__params:\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__param_exists_and_requires_grad(module, param):\n",
      "\u001b[0;31mAssertionError\u001b[0m: BackPACK extension expects a backpropagation quantity but it is None. Module: Component order:\tZNE\nSeisBench model\t\tEQTransformer\n\nEQTransformer(\n  (encoder): Encoder(\n    (convs): ModuleList(\n      (0): Conv1d(3, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      (1): Conv1d(8, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (3): Conv1d(16, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (5): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (6): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n    )\n    (pools): ModuleList(\n      (0-6): 7 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (res_cnn_stack): ResCNNStack(\n    (members): ModuleList(\n      (0-3): 4 x ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (4): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n      (5): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n      (6): ResCNNBlock(\n        (dropout): SpatialDropout1d(\n          (dropout): Dropout2d(p=0.1, inplace=False)\n        )\n        (norm1): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv1): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n        (norm2): BatchNorm1d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (conv2): Conv1d(64, 64, kernel_size=(2,), stride=(1,))\n      )\n    )\n  )\n  (bi_lstm_stack): BiLSTMStack(\n    (members): ModuleList(\n      (0-2): 3 x BiLSTMBlock(\n        (lstm): CustomLSTM(\n          (cell_f): ActivationLSTMCell()\n          (cell_b): ActivationLSTMCell()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (conv): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n        (norm): BatchNorm1d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (transformer_d0): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (transformer_d): Transformer(\n    (attention): SeqSelfAttention()\n    (norm1): LayerNormalization()\n    (ff): FeedForward(\n      (lin1): Linear(in_features=16, out_features=128, bias=True)\n      (lin2): Linear(in_features=128, out_features=16, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (norm2): LayerNormalization()\n  )\n  (decoder_d): Decoder(\n    (upsample): Upsample(scale_factor=2.0, mode='nearest')\n    (convs): ModuleList(\n      (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n      (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n      (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n      (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n      (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n      (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n      (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n    )\n  )\n  (conv_d): Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  (dropout): Dropout(p=0.1, inplace=False)\n  (pick_lstms): ModuleList(\n    (0-1): 2 x CustomLSTM(\n      (cell_f): ActivationLSTMCell()\n    )\n  )\n  (pick_attentions): ModuleList(\n    (0-1): 2 x SeqSelfAttention()\n  )\n  (pick_decoders): ModuleList(\n    (0-1): 2 x Decoder(\n      (upsample): Upsample(scale_factor=2.0, mode='nearest')\n      (convs): ModuleList(\n        (0): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n        (1): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n        (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n        (3): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n        (4): Conv1d(32, 16, kernel_size=(7,), stride=(1,), padding=(3,))\n        (5): Conv1d(16, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n        (6): Conv1d(16, 8, kernel_size=(11,), stride=(1,), padding=(5,))\n      )\n    )\n  )\n  (pick_convs): ModuleList(\n    (0-1): 2 x Conv1d(8, 1, kernel_size=(11,), stride=(1,), padding=(5,))\n  )\n), Extension: <backpack.extensions.secondorder.diag_ggn.DiagGGNExact object at 0x7eff04607050>."
     ]
    }
   ],
   "source": [
    "la.fit(train_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
